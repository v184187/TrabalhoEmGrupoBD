{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a976329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cff6cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-18.db  20-24.db     \u001b[0m\u001b[01;34mFuncionou\u001b[0m/          TentandoCriarDatabase.ipynb  tudo.db\r\n",
      "19.db    Completo.db  merged_database.db  \u001b[01;34mTrabalhoEmGrupoBD\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcd55b",
   "metadata": {},
   "source": [
    "# Planejamento\n",
    "Preciso juntar todos os dados em uma tabela que tenha ano, regiao, estado, estacao  como colunas e esteja so com informacoes diarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05a5e87",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:27\u001b[0;36m\u001b[0m\n\u001b[0;31m    try:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def read_dataset_with_metadata(file_path):\n",
    "    \"\"\"Reads a dataset with metadata.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            metadata_lines = [next(f) for x in range(8)]\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Skipping file {file_path} due to UnicodeDecodeError.\")\n",
    "        return None   \n",
    "    except StopIteration:\n",
    "        print(f\"Skipping file {file_path} due to incomplete metadata.\")\n",
    "        return None  \n",
    "\n",
    "    try:\n",
    "        metadata = {\n",
    "            'regiao': metadata_lines[0].split(';')[1].strip(),\n",
    "            'uf': metadata_lines[1].split(';')[1].strip(),\n",
    "            'estacao': metadata_lines[2].split(';')[1].strip(),\n",
    "            'codigo_wmo': metadata_lines[3].split(';')[1].strip(),\n",
    "            'latitude': float(metadata_lines[4].split(';')[1].strip().replace(',', '.')),\n",
    "            'longitude': float(metadata_lines[5].split(';')[1].strip().replace(',', '.')),\n",
    "            'altitude': float(metadata_lines[6].split(';')[1].strip().replace(',', '.')),\n",
    "            'data_fundacao': metadata_lines[7].split(';')[1].strip()\n",
    "        }\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\" {file_path}   {e}\")\n",
    "        return None   \n",
    "     try:\n",
    "        data = pd.read_csv(file_path,\n",
    "                           sep=';',\n",
    "                           encoding='latin-1',\n",
    "                           skiprows=8,\n",
    "                           decimal=',')\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"{file_path}\")\n",
    "        return None   \n",
    "     \n",
    "    data = data.replace(-9999, np.nan)\n",
    "\n",
    "    try:\n",
    "        data[['ano', 'mes', 'dia']] = data['DATA (YYYY-MM-DD)'].str.split('-', expand=True)\n",
    "    except KeyError:\n",
    "        print(f\"{file_path}\")\n",
    " \n",
    "    for key, value in metadata.items():\n",
    "        data[key] = value\n",
    "\n",
    "    aggregation_dict = {\n",
    "        'HORA (UTC)': 'first',  \n",
    "        'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)': 'mean',\n",
    "        'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': 'mean',\n",
    "        'PRESSÃO ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB)': 'max',\n",
    "        'PRESSÃO ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB)': 'min',\n",
    "        'RADIACAO GLOBAL (KJ/m²)': 'sum',\n",
    "        'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)': 'mean',\n",
    "        'TEMPERATURA DO PONTO DE ORVALHO (°C)': 'mean',\n",
    "        'TEMPERATURA MÁXIMA NA HORA ANT. (AUT) (°C)': 'max',\n",
    "        'TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)': 'min',\n",
    "        'TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (°C)': 'max',\n",
    "        'TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (°C)': 'min',\n",
    "        'UMIDADE REL. MAX. NA HORA ANT. (AUT) (%)': 'max',\n",
    "        'UMIDADE REL. MIN. NA HORA ANT. (AUT) (%)': 'min',\n",
    "        'UMIDADE RELATIVA DO AR, HORARIA (%)': 'mean',\n",
    "        'VENTO, DIREÇÃO HORARIA (gr) (° (gr))': 'mean',\n",
    "        'VENTO, RAJADA MAXIMA (m/s)': 'max',\n",
    "        'VENTO, VELOCIDADE HORARIA (m/s)': 'mean',\n",
    "        'Unnamed: 19': 'first',\n",
    "        'ano': 'first', \n",
    "        'mes': 'first',\n",
    "        'dia': 'first',\n",
    "        'estacao': 'first',\n",
    "        'regiao': 'first',\n",
    "        'uf': 'first',\n",
    "        'codigo_wmo': 'first',\n",
    "        'longitude': 'first',\n",
    "        'latitude': 'first',\n",
    "        'altitude': 'first',\n",
    "        'data_fundacao': 'first'\n",
    "    }\n",
    "\n",
    "    data = data.groupby('DATA (YYYY-MM-DD)', as_index=False).agg(aggregation_dict)\n",
    "\n",
    "    data.columns = [\n",
    "        'data',\n",
    "        'hora_utc',\n",
    "        'precipitacao_total_diaria',\n",
    "        'pressao_media',\n",
    "        'pressao_max',\n",
    "        'pressao_min',\n",
    "        'radiacao_total_diaria',\n",
    "        'temp_ar_media',\n",
    "        'temp_orvalho_media',\n",
    "        'temp_max',\n",
    "        'temp_min',\n",
    "        'temp_orvalho_max',\n",
    "        'temp_orvalho_min',\n",
    "        'umidade_max',\n",
    "        'umidade_min',\n",
    "        'umidade_media',\n",
    "        'vento_direcao_media',\n",
    "        'vento_rajada_max',\n",
    "        'vento_velocidade_media',\n",
    "        'col_anonima',\n",
    "        'ano',\n",
    "        'mes',\n",
    "        'dia',\n",
    "        'estacao',\n",
    "        'regiao',\n",
    "        'uf',\n",
    "        'codigo_wmo',\n",
    "        'longitude',\n",
    "        'latitude',\n",
    "        'altitude',\n",
    "        'data_fundacao'\n",
    "    ]\n",
    "\n",
    "    return {'metadata': metadata, 'data': data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"TrabalhoEmGrupoBD/2016\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.CSV')]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    result = read_dataset_with_metadata(file_path)\n",
    "\n",
    "    if result is not None:\n",
    "        all_data.append(result['data']) \n",
    "\n",
    "result = pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb28062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cf1f3",
   "metadata": {},
   "source": [
    "Juntando anos em uma pasta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abe85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = \"0-18.db\"\n",
    "\n",
    "all_data = []\n",
    "for year in range(2000, 2019):\n",
    "    folder_path = f\"TrabalhoEmGrupoBD/{year}\"\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.CSV')]\n",
    "    \n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        result = read_dataset_with_metadata(file_path)\n",
    "        \n",
    "        if result is not None:\n",
    "            all_data.append(result['data'])  \n",
    "\n",
    "result = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "result.to_sql(\"result_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b331864",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab97562",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_database(\"SELECT * FROM result_table\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9e296",
   "metadata": {},
   "source": [
    "## 2019 arrombado eh um ano c as colunas diferentes de tds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_dataset_with_metadata(file_path):\n",
    "    \"\"\"Reads a dataset with metadata.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            metadata_lines = [next(f) for x in range(8)]\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"{file_path}\")\n",
    "        return None  \n",
    "    except StopIteration:\n",
    "        print(f\"{file_path}\")\n",
    "        return None  \n",
    "\n",
    "\n",
    "    try:\n",
    "        metadata = {\n",
    "            'regiao': metadata_lines[0].split(';')[1].strip(),\n",
    "            'uf': metadata_lines[1].split(';')[1].strip(),\n",
    "            'estacao': metadata_lines[2].split(';')[1].strip(),\n",
    "            'codigo_wmo': metadata_lines[3].split(';')[1].strip(),\n",
    "            'latitude': float(metadata_lines[4].split(';')[1].strip().replace(',', '.')),\n",
    "            'longitude': float(metadata_lines[5].split(';')[1].strip().replace(',', '.')),\n",
    "            'altitude': float(metadata_lines[6].split(';')[1].strip().replace(',', '.')),\n",
    "            'data_fundacao': metadata_lines[7].split(';')[1].strip()\n",
    "        }\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Skipping file {file_path} due to metadata parsing error: {e}\")\n",
    "        return None  \n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path,\n",
    "                           sep=';',\n",
    "                           encoding='latin-1',\n",
    "                           skiprows=8,\n",
    "                           decimal=',')\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"{file_path}\")\n",
    "        return None  \n",
    "    required_columns = ['Data', 'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', 'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)', \n",
    "                        'RADIACAO GLOBAL (KJ/m²)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)', 'UMIDADE RELATIVA DO AR, HORARIA (%)']\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Skipping file {file_path} due to missing columns: {', '.join(missing_columns)}\")\n",
    "        return None \n",
    "\n",
    "    try:\n",
    "        data[['ano', 'mes', 'dia']] = data['Data'].str.split('/', expand=True)  \n",
    "    except KeyError:\n",
    "        print(f\"Skipping file {file_path} due to missing 'Data' column.\")\n",
    "        return None  \n",
    "\n",
    "    for key, value in metadata.items():\n",
    "        data[key] = value\n",
    "    aggregation_dict = {\n",
    "        'Hora UTC': 'first',  \n",
    "        'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)': 'mean',\n",
    "        'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': 'mean',\n",
    "        'PRESSÃO ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB)': 'max',\n",
    "        'PRESSÃO ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB)': 'min',\n",
    "        '''RADIACAO GLOBAL (KJ/m²)''': 'sum',\n",
    "        'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)': 'mean',\n",
    "        'TEMPERATURA DO PONTO DE ORVALHO (°C)': 'mean',\n",
    "        'TEMPERATURA MÁXIMA NA HORA ANT. (AUT) (°C)': 'max',\n",
    "        'TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)': 'min',\n",
    "        'TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (°C)': 'max',\n",
    "        'TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (°C)': 'min',\n",
    "        'UMIDADE REL. MAX. NA HORA ANT. (AUT) (%)': 'max',\n",
    "        'UMIDADE REL. MIN. NA HORA ANT. (AUT) (%)': 'min',\n",
    "        'UMIDADE RELATIVA DO AR, HORARIA (%)': 'mean',\n",
    "        'VENTO, DIREÇÃO HORARIA (gr) (° (gr))': 'mean',\n",
    "        'VENTO, RAJADA MAXIMA (m/s)': 'max',\n",
    "        'VENTO, VELOCIDADE HORARIA (m/s)': 'mean',\n",
    "        'Unnamed: 19': 'first',\n",
    "        'ano': 'first', \n",
    "        'mes': 'first',\n",
    "        'dia': 'first',\n",
    "        'estacao': 'first',\n",
    "        'regiao': 'first',\n",
    "        'uf': 'first',\n",
    "        'codigo_wmo': 'first',\n",
    "        'longitude': 'first',\n",
    "        'latitude': 'first',\n",
    "        'altitude': 'first',\n",
    "        'data_fundacao': 'first'\n",
    "    }\n",
    "\n",
    "    data = data.groupby('Data', as_index=False).agg(aggregation_dict)\n",
    "\n",
    "   \n",
    "    data.columns = [\n",
    "        'data',\n",
    "        'hora_utc',\n",
    "        'precipitacao_total_diaria',\n",
    "        'pressao_media',\n",
    "        'pressao_max',\n",
    "        'pressao_min',\n",
    "        'radiacao_total_diaria',\n",
    "        'temp_ar_media',\n",
    "        'temp_orvalho_media',\n",
    "        'temp_max',\n",
    "        'temp_min',\n",
    "        'temp_orvalho_max',\n",
    "        'temp_orvalho_min',\n",
    "        'umidade_max',\n",
    "        'umidade_min',\n",
    "        'umidade_media',\n",
    "        'vento_direcao_media',\n",
    "        'vento_rajada_max',\n",
    "        'vento_velocidade_media',\n",
    "        'col_anonima',  \n",
    "        'ano',\n",
    "        'mes',\n",
    "        'dia',\n",
    "        'estacao',\n",
    "        'regiao',\n",
    "        'uf',\n",
    "        'codigo_wmo',\n",
    "        'longitude',\n",
    "        'latitude',\n",
    "        'altitude',\n",
    "        'data_fundacao'\n",
    "    ]\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297b346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b76e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = \"19.db\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in range(2019, 2020):\n",
    "  \n",
    "    folder_path = f\"TrabalhoEmGrupoBD/{year}\"\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.CSV')]\n",
    "    \n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        result = read_dataset_with_metadata(file_path)\n",
    "        \n",
    "        if result is not None:\n",
    "            all_data.append(result['data'])  \n",
    "\n",
    "result = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "result.to_sql(\"result_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144830e",
   "metadata": {},
   "source": [
    "# Agora lendo os pos 2019, que mudam algumas coisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_dataset_with_metadata(file_path\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            metadata_lines = [next(f) for x in range(8)]\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"{file_path}\")\n",
    "        return None  \n",
    "    except StopIteration:\n",
    "        print(f\"{file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        metadata = {\n",
    "            'regiao': metadata_lines[0].split(';')[1].strip(),\n",
    "            'uf': metadata_lines[1].split(';')[1].strip(),\n",
    "            'estacao': metadata_lines[2].split(';')[1].strip(),\n",
    "            'codigo_wmo': metadata_lines[3].split(';')[1].strip(),\n",
    "            'latitude': float(metadata_lines[4].split(';')[1].strip().replace(',', '.')),\n",
    "            'longitude': float(metadata_lines[5].split(';')[1].strip().replace(',', '.')),\n",
    "            'altitude': float(metadata_lines[6].split(';')[1].strip().replace(',', '.')),\n",
    "            'data_fundacao': metadata_lines[7].split(';')[1].strip()\n",
    "        }\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Skipping file {file_path} due to metadata parsing error: {e}\")\n",
    "        return None \n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path,\n",
    "                           sep=';',\n",
    "                           encoding='latin-1',\n",
    "                           skiprows=8,\n",
    "                           decimal=',')\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\" {file_path}\")\n",
    "        return None  \n",
    "\n",
    "    required_columns = ['Data', 'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', 'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)', \n",
    "                        'RADIACAO GLOBAL (Kj/m²)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)', 'UMIDADE RELATIVA DO AR, HORARIA (%)']\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"{file_path} columns: {', '.join(missing_columns)}\")\n",
    "        return None  \n",
    "\n",
    "    \n",
    "    try:\n",
    "        data[['ano', 'mes', 'dia']] = data['Data'].str.split('/', expand=True)\n",
    "    except KeyError:\n",
    "        print(f\"{file_path} \")\n",
    "        return None  \n",
    "\n",
    "    for key, value in metadata.items():\n",
    "        data[key] = value\n",
    "\n",
    "    aggregation_dict = {\n",
    "        'Hora UTC': 'first',  \n",
    "        'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)': 'mean',\n",
    "        'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': 'mean',\n",
    "        'PRESSÃO ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB)': 'max',\n",
    "        'PRESSÃO ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB)': 'min',\n",
    "        '''RADIACAO GLOBAL (Kj/m²)''': 'sum',\n",
    "        'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)': 'mean',\n",
    "        'TEMPERATURA DO PONTO DE ORVALHO (°C)': 'mean',\n",
    "        'TEMPERATURA MÁXIMA NA HORA ANT. (AUT) (°C)': 'max',\n",
    "        'TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)': 'min',\n",
    "        'TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (°C)': 'max',\n",
    "        'TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (°C)': 'min',\n",
    "        'UMIDADE REL. MAX. NA HORA ANT. (AUT) (%)': 'max',\n",
    "        'UMIDADE REL. MIN. NA HORA ANT. (AUT) (%)': 'min',\n",
    "        'UMIDADE RELATIVA DO AR, HORARIA (%)': 'mean',\n",
    "        'VENTO, DIREÇÃO HORARIA (gr) (° (gr))': 'mean',\n",
    "        'VENTO, RAJADA MAXIMA (m/s)': 'max',\n",
    "        'VENTO, VELOCIDADE HORARIA (m/s)': 'mean',\n",
    "        'Unnamed: 19': 'first',\n",
    "        'ano': 'first', \n",
    "        'mes': 'first',\n",
    "        'dia': 'first',\n",
    "        'estacao': 'first',\n",
    "        'regiao': 'first',\n",
    "        'uf': 'first',\n",
    "        'codigo_wmo': 'first',\n",
    "        'longitude': 'first',\n",
    "        'latitude': 'first',\n",
    "        'altitude': 'first',\n",
    "        'data_fundacao': 'first'\n",
    "    }\n",
    "\n",
    "    data = data.groupby('Data', as_index=False).agg(aggregation_dict)\n",
    "\n",
    "    data.columns = [\n",
    "        'data',\n",
    "        'hora_utc',\n",
    "        'precipitacao_total_diaria',\n",
    "        'pressao_media',\n",
    "        'pressao_max',\n",
    "        'pressao_min',\n",
    "        'radiacao_total_diaria',\n",
    "        'temp_ar_media',\n",
    "        'temp_orvalho_media',\n",
    "        'temp_max',\n",
    "        'temp_min',\n",
    "        'temp_orvalho_max',\n",
    "        'temp_orvalho_min',\n",
    "        'umidade_max',\n",
    "        'umidade_min',\n",
    "        'umidade_media',\n",
    "        'vento_direcao_media',\n",
    "        'vento_rajada_max',\n",
    "        'vento_velocidade_media',\n",
    "        'col_anonima', \n",
    "        'ano',\n",
    "        'mes',\n",
    "        'dia',\n",
    "        'estacao',\n",
    "        'regiao',\n",
    "        'uf',\n",
    "        'codigo_wmo',\n",
    "        'longitude',\n",
    "        'latitude',\n",
    "        'altitude',\n",
    "        'data_fundacao'\n",
    "    ]\n",
    "\n",
    "    return {'metadata': metadata, 'data': data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42453703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = \"20-24.db\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in range(2020, 2025):\n",
    "    folder_path = f\"TrabalhoEmGrupoBD/{year}\"\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.CSV')]\n",
    "    \n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        result = read_dataset_with_metadata(file_path)\n",
    "        \n",
    "        if result is not None:\n",
    "            all_data.append(result['data'])  \n",
    "\n",
    "result = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "result.to_sql(\"result_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692faaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f5c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce023af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_from_db(db_file):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    tables = pd.read_sql(query, conn)\n",
    "    print(f\"Tables in {db_file}: {tables}\")\n",
    "    \n",
    "    table_name = tables.iloc[0, 0]\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def save_data_to_new_db(new_db_file, df, table_name):\n",
    "    conn = sqlite3.connect(new_db_file)\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.close()\n",
    "\n",
    "new_db_file = 'merged_database.db'\n",
    "\n",
    "df_0_18 = load_data_from_db('0-18.db')\n",
    "df_19 = load_data_from_db('19.db')\n",
    "df_20_24 = load_data_from_db('20-24.db')\n",
    "\n",
    "save_data_to_new_db(new_db_file, df_0_18, 'table_0_18')\n",
    "save_data_to_new_db(new_db_file, df_19, 'table_19')\n",
    "save_data_to_new_db(new_db_file, df_20_24, 'table_20_24')\n",
    "\n",
    "print(f\" {new_db_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('0-18.db')\n",
    "conn1 = sqlite3.connect('19.db')\n",
    "conn2 = sqlite3.connect('20-24.db')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_database(\"SELECT * FROM result_table\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn_0_18 = sqlite3.connect('0-18.db')\n",
    "conn_19 = sqlite3.connect('19.db')\n",
    "conn_20_24 = sqlite3.connect('20-24.db')\n",
    "\n",
    "conn_merged = sqlite3.connect('merged_database.db')\n",
    "cursor_merged = conn_merged.cursor()\n",
    "\n",
    "cursor_0_18 = conn_0_18.cursor()\n",
    "cursor_0_18.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "table_name = cursor_0_18.fetchone()[0]  \n",
    "\n",
    "cursor_0_18.execute(f\"PRAGMA table_info({table_name});\")\n",
    "columns = [col[1] for col in cursor_0_18.fetchall()]  \n",
    "columns_str = \", \".join(columns) \n",
    "\n",
    "cursor_merged.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns_str});\")\n",
    "\n",
    "def copy_data(source_conn, source_table, target_conn):\n",
    "    source_cursor = source_conn.cursor()\n",
    "    source_cursor.execute(f\"SELECT * FROM {source_table}\")\n",
    "    rows = source_cursor.fetchall()\n",
    "\n",
    "    placeholders = \", \".join([\"?\"] * len(columns))\n",
    "    target_cursor = target_conn.cursor()\n",
    "    target_cursor.executemany(f\"INSERT INTO {source_table} VALUES ({placeholders})\", rows)\n",
    "    target_conn.commit()\n",
    "\n",
    "copy_data(conn_0_18, table_name, conn_merged)\n",
    "copy_data(conn_19, table_name, conn_merged)\n",
    "copy_data(conn_20_24, table_name, conn_merged)\n",
    "\n",
    "conn_0_18.close()\n",
    "conn_19.close()\n",
    "conn_20_24.close()\n",
    "conn_merged.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('merged_database.db')\n",
    "pl.read_database(\"SELECT * FROM result_table \",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
